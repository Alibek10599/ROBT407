{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "Homework-3.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "go9D0wQzv5L_",
        "QnzBomAYv5MD",
        "jv7Q6W08v5MK",
        "t-rK-UGCv5Mf",
        "2Q_cODdov5Mf",
        "uMGD4l7Qv5M1",
        "-zJsqWh3v5M4",
        "H126kezZv5M5",
        "LQaHQZwBv5NA",
        "-0KKU5Nwv5Nq",
        "ne9QoO8Ev5Nv",
        "5S3Q2V82v5Nx",
        "CMSRuOTnv5N5",
        "eLaq9p37v5N6",
        "zXZiEztjv5N_",
        "GOJIF1U_v5OF",
        "QgL2FlQ6v5OO",
        "twEBABbjv5OR",
        "hAB8jV8rv5OS",
        "oyrqFc4qv5Og",
        "LhqUk6wDv5Ot",
        "PYiO8RxZv5Oy"
      ],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diER8HhHv5Lg",
        "colab_type": "text"
      },
      "source": [
        "------\n",
        "\n",
        "# HOMEWORK 3\n",
        "\n",
        "### Instructions\n",
        "This homework is based on Neural Networks and Pytorch deep learning framework.\n",
        "After finishing this homework, you will be able to train a multilayer neural network using Pytorch, and solve image recognition/classification task.\n",
        "\n",
        "You have to read and run all the cell in the provided notebook, and then work on the assigments. \n",
        "\n",
        "There are four sections on the notebook identified as \n",
        "- PART 1 - What is Pytorch? \n",
        "- PART 2 - Automatic Differentiation \n",
        "- PART 3 - Multilayer Perceptrons  \n",
        "- PART 4 - MNIST - Feed-Forward Neural Networks\n",
        "\n",
        "At the end of each part, you can find your assignments which you are supposed to work on. You will need to create a cell after each assignment and provide your solutions.\n",
        "\n",
        "Submit your solutions as a jupyter ***.ipynb*** notebooks to the moodle. \n",
        "\n",
        "***Due date: April 23, midnight***\n",
        "\n",
        "You can use https://colab.research.google.com/, or local laptop to work on these tasks. \n",
        "\n",
        "\n",
        "---------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efpr4E-pv5Li",
        "colab_type": "text"
      },
      "source": [
        "# PART 1 - What is PyTorch?\n",
        "\n",
        "\n",
        "It’s a Python based scientific computing package targeted at two sets of\n",
        "audiences:\n",
        "-  A replacement for numpy to use the power of GPUs\n",
        "-  a deep learning research platform that provides maximum flexibility\n",
        "   and speed  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Bt_TMCBv5Lj",
        "colab_type": "text"
      },
      "source": [
        "### Director of AI at Tesla\n",
        "\n",
        "![Andrei-Karpathy](https://www.dropbox.com/s/dt2vvxafq1owehy/andrei-karpathy.png?dl=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50dCmDjSv5Lk",
        "colab_type": "text"
      },
      "source": [
        "# Getting Started\n",
        "\n",
        "In this notebook you will get a quick start on what pytorch is and how to use it.\n",
        "\n",
        "## 1. Tensors\n",
        "\n",
        "Tensors are similar to numpy’s ndarrays, with the addition being that\n",
        "Tensors can also be used on a GPU to accelerate computing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSJQljJev5Lk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75nWxS9fv5Lo",
        "colab_type": "text"
      },
      "source": [
        "Construct a 5x3 matrix, uninitialized"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4n6lUxlvv5Lp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = torch.Tensor(5, 3)\n",
        "print(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHg80Smdv5Lt",
        "colab_type": "text"
      },
      "source": [
        "Construct a randomly initialized matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91j_7dYRv5Lt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = torch.rand(5, 3)\n",
        "print(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNI2ATdDv5Lx",
        "colab_type": "text"
      },
      "source": [
        "Get its size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2FDiF0Iv5Ly",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(x.size())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iefi8sejv5L2",
        "colab_type": "text"
      },
      "source": [
        "**NOTE** `torch.Size` is in fact a tuple, so it supports the same operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkyY7G1Sv5L2",
        "colab_type": "text"
      },
      "source": [
        "----\n",
        "### Part 1 -- Assignment (a)\n",
        "\n",
        "1. Make a tensor of size (2, 17)\n",
        "2. Make a torch.FloatTensor of size (3, 1)\n",
        "3. Make a torch.LongTensor of size (5, 2, 1)\n",
        "  - fill the entire tensor with 7s\n",
        "4. Make a torch.ByteTensor of size (5,)\n",
        "  - fill the middle 3 indices with ones such that it records [0, 1, 1, 1, 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsuXjY6Lv5L3",
        "colab_type": "text"
      },
      "source": [
        "-----------------\n",
        "## 2. Operations\n",
        "There are multiple syntaxes for operations. Let's see addition as an example:\n",
        "\n",
        "### 2.1 Addition: syntax 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMGZKCYQv5L4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = torch.rand(5, 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJFL7jOkv5L8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(x + y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "go9D0wQzv5L_",
        "colab_type": "text"
      },
      "source": [
        "### 2.2 Addition: syntax 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tb6idEIgv5L_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(torch.add(x, y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnzBomAYv5MD",
        "colab_type": "text"
      },
      "source": [
        "### 2.3 Addition: giving an output tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oc1pKzz6v5ME",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = torch.Tensor(5, 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ie7h_EMHv5MH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.add(x, y, out = result)\n",
        "print(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jv7Q6W08v5MK",
        "colab_type": "text"
      },
      "source": [
        "### 2.4 Addition: in-place\n",
        "\n",
        "adds `x`to `y`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gdzhjapv5ML",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KJUsaQLv5MO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y.add_(x)\n",
        "print(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0nG6Vnyv5MR",
        "colab_type": "text"
      },
      "source": [
        "**NOTE** Any operation that mutates a tensor in-place is post-fixed with an `_`. For example: `x.copy_(y)`, `x.t_()`, will change `x`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNKcvhdrv5MR",
        "colab_type": "text"
      },
      "source": [
        "You can use standard numpy-like indexing with all bells and whistles!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHbTgdYlv5MS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(x[:, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rf5HoNfAv5MV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(x.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAJUzIsCv5MY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EPJGJUfv5Mb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y.mul_?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZtnSX9pv5Me",
        "colab_type": "text"
      },
      "source": [
        "**Read later** 100+ Tensor operations, including transposing, indexing, slicing, mathematical operations, linear algebra, random numbers, etc are described here <http://pytorch.org/docs/torch>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-rK-UGCv5Mf",
        "colab_type": "text"
      },
      "source": [
        "----\n",
        "### Part 1 - Assignment (b)\n",
        "\n",
        "1. multiply of two tensors (see [torch.Tensor.mul](http://pytorch.org/docs/master/tensors.html#torch.Tensor.mul))\n",
        "2. do the same, but inplace\n",
        "3. division of two tensors (see [torch.Tensor.div](http://pytorch.org/docs/master/tensors.html#torch.Tensor.div))\n",
        "4. perform a matrix multiplication of two tensors of size (2, 4) and (4, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Q_cODdov5Mf",
        "colab_type": "text"
      },
      "source": [
        "---------\n",
        "## 3. Numpy Bridge\n",
        "\n",
        "Converting a torch Tensor to a numpy array and vice versa is a breeze.\n",
        "\n",
        "The torch Tensor and numpy array will share their underlying memory locations, and changing one will change the other.\n",
        "\n",
        "### 3.1 Converting torch Tensor to numpy Array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gos7UqVfv5Mg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = torch.ones(5)\n",
        "print(a)\n",
        "type(a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4s8-vXihv5Mj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9Yd3W7ev5Ml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "type(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4Vrki8dv5Mo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x.numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKvyCvJpv5Mr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b = a.numpy()\n",
        "\n",
        "print(b)\n",
        "type(b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_Bnt8-Bv5Mt",
        "colab_type": "text"
      },
      "source": [
        "See how the numpy array changed in value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fT12Pz5Jv5Mu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# in place addition \n",
        "a.add_(1)\n",
        "\n",
        "print(a)\n",
        "print(b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ceo1QMev5Mx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMGD4l7Qv5M1",
        "colab_type": "text"
      },
      "source": [
        "### 3.2 Converting numpy Array to torch Tensor\n",
        "\n",
        "See how changing the np array changed the torch Tensor automatically"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZG0mBGnv5M1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "xx = np.random.rand(16, 153, 100)\n",
        "print(xx.shape)\n",
        "\n",
        "# convert to a pytorch tensor \n",
        "b = torch.from_numpy(xx)\n",
        "print(b.shape)\n",
        "\n",
        "# add ones - this happens in place \n",
        "a1  = np.add(a.numpy(), 1)\n",
        "type(a1)\n",
        "\n",
        "a2 = torch.from_numpy(a1)\n",
        "type(a2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zJsqWh3v5M4",
        "colab_type": "text"
      },
      "source": [
        "---- \n",
        "### Part 1 - Assignment (c)\n",
        "\n",
        "1. create a tensor of size (5, 2) containing ones\n",
        "2. now convert it to a numpy array\n",
        "3. now convert it back to a torch tensor\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H126kezZv5M5",
        "colab_type": "text"
      },
      "source": [
        "All the Tensors on the CPU except a CharTensor support converting to NumPy and back.\n",
        "\n",
        "## 4 CUDA Tensors\n",
        "\n",
        "Tensors can be moved onto GPU using the `.cuda` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLF4cCjlv5M5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kF7w1tzCv5M7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# let us run this cell only if CUDA is available\n",
        "if torch.cuda.is_available():    \n",
        "    x = x.cuda()\n",
        "    y = y.cuda()    \n",
        "    z = x + y\n",
        "    # notice that the tensors are now of type torch.cuda.FloatTensor (notice the cuda in there)\n",
        "    # this is meant as a tensor to be run on the GPU.\n",
        "    # the .cuda() does this to any parameter it is applied to\n",
        "    print(x)\n",
        "    print(y)\n",
        "    print(z)\n",
        "else:\n",
        "    print(\"CUDA not available on your machine.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtHwLisDv5M_",
        "colab_type": "text"
      },
      "source": [
        "-----\n",
        "\n",
        "# PART 2 - Autograd: automatic differentiation\n",
        "\n",
        "Central to all neural networks in PyTorch is the ``autograd`` package.\n",
        "Let’s first briefly visit this, and we will then go to training our first neural network.\n",
        "\n",
        "The `autograd` package provides automatic differentiation for all operations on Tensors.\n",
        "It is a define-by-run framework, which means that your backprop is defined by how your code is run, and that every single iteration can be different.\n",
        "\n",
        "Let us see this in more simple terms with some examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQaHQZwBv5NA",
        "colab_type": "text"
      },
      "source": [
        "## 1. Variable\n",
        "\n",
        "`autograd.Variable` is the central class of the package. It wraps a Tensor, and supports nearly all of operations defined on it.\n",
        "Once you finish your computation you can call `.backward()` and have all the gradients computed automatically.\n",
        "\n",
        "You can access the raw tensor through the `.data` attribute, while the gradient w.r.t. this variable is accumulated into `.grad`.\n",
        "\n",
        "There’s one more class which is very important for autograd implementation - a `Function`.\n",
        "\n",
        "`Variable` and `Function` are interconnected and build up an `acyclic graph`, that encodes a complete history of computation. Each variable has a `.grad_fn` attribute that references a `Function` that has created the `Variable` (except for `Variable`s created by the user - their `grad_fn` is `None`).\n",
        "\n",
        "If you want to compute the derivatives, you can call `.backward()` on a `Variable`. If `Variable` is a scalar (i.e. it holds a one element data), you don’t need to specify any arguments to `backward()`, however if it has more elements, you need to specify a `grad_output` argument that is a tensor of matching shape."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_W1Ghv5v5NA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3WCl8N_v5ND",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58SQwMwyv5NF",
        "colab_type": "text"
      },
      "source": [
        "Create a variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0leAXjcv5NF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f(x) = Wx + b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38XBXxF5v5NI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "W = Variable(torch.ones(2, 2), \n",
        "             requires_grad=True)\n",
        "print(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nM3prbjJv5NK",
        "colab_type": "text"
      },
      "source": [
        "Do an operation of variable:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RNZB9V_v5NL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = x + 2\n",
        "print(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDIew5kvv5NO",
        "colab_type": "text"
      },
      "source": [
        "`y` was created as a result of an operation, so it has a `grad_fn`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2S7FKit5v5NO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(y.grad_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCsuAy_5v5NR",
        "colab_type": "text"
      },
      "source": [
        "Do more operations on y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhBbbHCZv5NR",
        "colab_type": "text"
      },
      "source": [
        "$z = 3*(x+2)^2$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luLJNRbPv5NS",
        "colab_type": "text"
      },
      "source": [
        "y = x + 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OthlM28qv5NS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "z = 3* y * y  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1xaMYqav5NV",
        "colab_type": "text"
      },
      "source": [
        "$out = \\frac{1}{N}\\sum_{i}^{N} 3(x_{i} + 2)^2$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdA1We0cv5NV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out = z.mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSuQT-x_v5NY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(z)\n",
        "print(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUfXgBCpv5Na",
        "colab_type": "text"
      },
      "source": [
        "## 2. Gradients\n",
        "\n",
        "Let’s backprop now `out.backward()` is equivalent to doing `out.backward(torch.Tensor([1.0]))`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLZteEbzv5Nb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out.backward()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsLMzz2qv5Nd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(out)\n",
        "print(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7zW9Kikv5Ng",
        "colab_type": "text"
      },
      "source": [
        "print gradients d(out)/dx"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9FktoLwv5Nh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(x.grad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VD87kbEdv5Nj",
        "colab_type": "text"
      },
      "source": [
        "You should have got a matrix of `4.5`. Let’s call the `out` *Variable* $o$.\n",
        "\n",
        "We have that $o = \\frac{1}{4}\\sum_i z_i$,\n",
        "$z_i = 3(x_i+2)^2$ and $z_i\\bigr\\rvert_{x_i=1} = 27$.\n",
        "\n",
        "Therefore, $\\frac{\\partial o}{\\partial x_i} = \\frac{3}{2}(x_i+2)$,\n",
        "hence $\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5$.\n",
        "\n",
        "`You can do many crazy things with autograd!`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xP4REes2v5Nk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = torch.randn(3)\n",
        "x = Variable(x, requires_grad=True)\n",
        "\n",
        "y = x * 2\n",
        "while y.data.norm() < 1000:\n",
        "    y = y * 2\n",
        "\n",
        "print(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0TR1SIcv5Nm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gradients = torch.FloatTensor([0.1, 1.0, 0.0001])\n",
        "y.backward(gradients)\n",
        "\n",
        "print(x.grad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-MFF2Avv5No",
        "colab_type": "text"
      },
      "source": [
        "**Read later**\n",
        "Documentation of `Variable` and `Function` is at http://pytorch.org/docs/autograd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q37_Rd_v5No",
        "colab_type": "text"
      },
      "source": [
        "----------\n",
        "### Part 2 - Assignments (a)\n",
        "\n",
        "1. Define a tensor\n",
        "2. Convert the tensor to a torch.Variable that requires_grad\n",
        "3. Multiply the torch.Variable with 2 and assign the result to a new python variable (i.e. `x = result`)\n",
        "4. Sum the variable's elements and assign to a new python variable\n",
        "5. Print the gradients of all the variables\n",
        "6. Now perform a backward pass on the last variable\n",
        "7. Print all gradients again\n",
        "  - what did you notice?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cERr4wbPv5Np",
        "colab_type": "text"
      },
      "source": [
        "---------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNJzNf69v5Nq",
        "colab_type": "text"
      },
      "source": [
        "# PART 3 - MULTILAYER PERCEPTRONS\n",
        "In the following notebook you will implement a simple neural network in PyTorch.\n",
        "\n",
        "The building blocks of `PyTorch` are `Tensors`, `Variables` and `Operations`, with these we can form `dynamic computational graphs` that form neural networks.\n",
        "\n",
        "In this exercise we'll start right away by defining a logistic regression model using these simple building blocks.\n",
        "\n",
        "We'll initially start with a simple 2D and two-class classification problem where the class decision boundary can be visualized.\n",
        "\n",
        "Initially we show that logistic regression can only separate classes linearly.\n",
        "Adding a nonlinear hidden layer to the algorithm permits nonlinear class separation.\n",
        "\n",
        "In this notebook you should:\n",
        "* **First** run the code as is, and see what it does.\n",
        "* **Then** modify the code, following the instructions in the bottom of the notebook.\n",
        "* **Lastly** play a round a bit, and do some small experiments that you come up with.\n",
        "\n",
        "> We assume that you are already familiar with backpropagation (if not please see [Andrej Karpathy](http://cs.stanford.edu/people/karpathy/) or [Michal Nielsen](http://neuralnetworksanddeeplearning.com/chap2.html))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0KKU5Nwv5Nq",
        "colab_type": "text"
      },
      "source": [
        "## Dependancies and supporting functions\n",
        "Loading dependancies and supporting functions by running the code block below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXULXx5dv5Nr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.datasets\n",
        "\n",
        "# Do not worry about the code below for now, it is used for plotting later\n",
        "def plot_decision_boundary(pred_func, X, y):\n",
        "    #from https://github.com/dennybritz/nn-from-scratch/blob/master/nn-from-scratch.ipynb\n",
        "    # Set min and max values and give it some padding\n",
        "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
        "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
        "    \n",
        "    h = 0.01\n",
        "    # Generate a grid of points with distance h between them\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "    \n",
        "    yy = yy.astype('float32')\n",
        "    xx = xx.astype('float32')\n",
        "    \n",
        "    # Predict the function value for the whole gid\n",
        "    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])[:,0]\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    # Plot the contour and training examples\n",
        "    plt.figure()\n",
        "    plt.contourf(xx, yy, Z, cmap=plt.cm.RdBu)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=-y, cmap=plt.cm.Spectral)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhRqVuFnv5Nt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def onehot(t, num_classes):    \n",
        "    out = np.zeros((t.shape[0], num_classes))    \n",
        "    for row, col in enumerate(t):\n",
        "        out[row, col] = 1\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ne9QoO8Ev5Nv",
        "colab_type": "text"
      },
      "source": [
        "## Problem \n",
        "We'll initally demonstrate the that Multi-layer Perceptrons (MLPs) can classify nonlinear problems, whereas simple logistic regression cannot.\n",
        "For ease of visualization and computationl speed we initially experiment on the simple 2D half-moon dataset, visualized below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd46IyNMv5Nv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate a dataset and plot it\n",
        "np.random.seed(0)\n",
        "num_samples = 300\n",
        "\n",
        "X, y = sklearn.datasets.make_moons(num_samples, noise=0.20)\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S3Q2V82v5Nx",
        "colab_type": "text"
      },
      "source": [
        "### Define train, validation, and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdGSIBbsv5Nx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_tr = X[:100].astype('float32')\n",
        "print('Train set:', X_tr.shape)\n",
        "\n",
        "X_val = X[100:200].astype('float32')\n",
        "print('Validation set:', X_val.shape)\n",
        "\n",
        "X_te = X[200:].astype('float32')\n",
        "print('Test set:', X_te.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd6qy_Z_v5N0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# and labels\n",
        "y_tr = y[:100].astype('int32')\n",
        "y_val = y[100:200].astype('int32')\n",
        "y_te = y[200:].astype('int32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQleUdcwv5N3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.scatter(X_tr[:,0], X_tr[:,1], s=40, c=y_tr, cmap=plt.cm.Spectral)\n",
        "print(X.shape, y.shape)\n",
        "\n",
        "num_features = X_tr.shape[-1]\n",
        "print('Number of features:>>', num_features)\n",
        "\n",
        "num_output = 2\n",
        "print('Number of outputs:>>', num_output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMSRuOTnv5N5",
        "colab_type": "text"
      },
      "source": [
        "## From Logistic Regression to \"Deep Learning\"\n",
        "The code implements logistic regression. In section [__Assignments Half Moon__](#Assignments-Half-Moon) (bottom of this notebook) you are asked to modify the code into a neural network.\n",
        "\n",
        "The standard building block for neural networks are layers.\n",
        "The simplest of which is called a `dense feed forward layer`, and it is computed as follows:\n",
        "\n",
        "$$y = \\mathrm{nonlinearity}(xW + b)$$\n",
        "\n",
        "where $x$ is the input vector, $y$ is the output vector, and $W, b$ are the weights (a matrix and vector respectively).\n",
        "The *dense* part of the name comes from the fact that every element of $x$ contributes to every element of $y$.\n",
        "And the *feed forward* part of the name means that the layer processes each input independently. \n",
        "If we were to draw the layer it would be acyclical.\n",
        "Later we will se layers that break both of these conventions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRy5zaVGv5N6",
        "colab_type": "text"
      },
      "source": [
        "- $x$ has shape `[batch_size, num_features]`,\n",
        "- $W$ has shape `[num_features, num_units]`,\n",
        "- $b$ has `[num_units]`, and\n",
        "- $y$ has then `[batch_size, num_units]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLaq9p37v5N6",
        "colab_type": "text"
      },
      "source": [
        "## PyTorch 101\n",
        "\n",
        "In this first exercise we will use basic PyTorch functions so that you can learn how to build it from scratch. This will help you later if you want to build your own custom operations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpHvvqBEv5N7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.parameter import Parameter # A kind of Tensor that is to be considered a module parameter.\n",
        "\n",
        "import torch.nn as nn # Base class for all neural network modules.\n",
        "import torch.nn.functional as F # Functional interface  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9cHNrNnv5N_",
        "colab_type": "text"
      },
      "source": [
        "[`Parameters`](http://pytorch.org/docs/0.1.12/nn.html#torch.nn.Parameter) are [`Variable`](http://pytorch.org/docs/0.1.12/autograd.html#torch.autograd.Variable) subclasses, that have a very special property when used with [`Module`](http://pytorch.org/docs/0.1.12/nn.html#torch.nn.Module)'s - when they’re assigned as `Module` attributes they are automatically added to the list of its parameters, and will appear e.g. in [`.parameters()`](http://pytorch.org/docs/0.1.12/nn.html#torch.nn.Module.parameters) iterator.\n",
        "Assigning a Variable doesn’t have such effect.\n",
        "This is because one might want to cache some temporary state (more on this later) in the model.\n",
        "If there was no such class as `Parameter`, these temporaries would get registered too."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXZiEztjv5N_",
        "colab_type": "text"
      },
      "source": [
        "#### nn.Linear(in_features, out_features, bias=True)\n",
        "#### Applies a linear transformation to the incoming data: :math:`y = xA^T + b`\n",
        "\n",
        "Args:\n",
        "    in_features: size of each input sample\n",
        "    out_features: size of each output sample\n",
        "    bias: If set to False, the layer will not learn an additive bias.\n",
        "        Default: ``True``\n",
        "\n",
        "Shape:\n",
        "    - Input: :math:`(N, *, in\\_features)` where :math:`*` means any number of\n",
        "      additional dimensions\n",
        "    - Output: :math:`(N, *, out\\_features)` where all but the last dimension\n",
        "      are the same shape as the input.\n",
        "\n",
        "Attributes:\n",
        "    weight: the learnable weights of the module of shape\n",
        "        `(out_features x in_features)`\n",
        "    bias:   the learnable bias of the module of shape `(out_features)`\n",
        "\n",
        "Examples::\n",
        "\n",
        "    >>> m = nn.Linear(20, 30)\n",
        "    >>> input = torch.randn(128, 20)\n",
        "    >>> output = m(input)\n",
        "    >>> print(output.size())"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bX1dc_MFv5OA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LogisticRegression(nn.Module):    \n",
        "    def __init__(self):                 \n",
        "        super(LogisticRegression, self).__init__()              \n",
        "        self.W_1 = Parameter(torch.randn(num_output, num_features)) \n",
        "        self.b_1 = Parameter(torch.randn(num_output))\n",
        "        \n",
        "    def forward(self, x):             \n",
        "        # Applies a linear transformation :math:`y = xW^T + b` \n",
        "        x = F.linear(x, self.W_1, self.b_1)\n",
        "        x = F.softmax(x, dim = 1)\n",
        "        return x    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGWYtX2sv5OD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = LogisticRegression()\n",
        "print(net)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOJIF1U_v5OF",
        "colab_type": "text"
      },
      "source": [
        "------\n",
        "## A softmax function.\n",
        "\n",
        "Softmax is defined as:\n",
        "\n",
        "$\\text{Softmax}(x_{i}) = \\frac{exp(x_i)}{\\sum_j exp(x_j)}$\n",
        "\n",
        "It is applied to all slices along dim, and will re-scale them so that the elements\n",
        "lie in the range `(0, 1)` and sum to 1.\n",
        "\n",
        "See :class:`~torch.nn.Softmax` for more details.\n",
        "\n",
        "Arguments:\n",
        "    input (Tensor): input\n",
        "    dim (int): A dimension along which softmax will be computed.\n",
        "\n",
        ".. note::\n",
        "    This function doesn't work directly with NLLLoss,\n",
        "    which expects the Log to be computed between the Softmax and itself.\n",
        "    Use log_softmax instead (it's faster and has better numerical properties).\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "of0kGvUpv5OF",
        "colab_type": "text"
      },
      "source": [
        "Knowing how to print your tensors is useful"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNV_y9ybv5OF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"NAMED PARAMETERS\")\n",
        "print(list(net.named_parameters()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyqO4_GAv5OH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the parameters() method simply gives the Tensors in the list\n",
        "print(\"PARAMETERS\")\n",
        "print(list(net.parameters()))\n",
        "print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4wPKEmWv5OK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# list individual parameters by name\n",
        "print('WEIGHTS')\n",
        "print(net.W_1)\n",
        "print(net.W_1.size())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaNfswkuv5OM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('\\nBIAS')\n",
        "print(net.b_1)\n",
        "print(net.b_1.size())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgL2FlQ6v5OO",
        "colab_type": "text"
      },
      "source": [
        "## Exploring Parameter\n",
        "\n",
        "Ok, let's investigate what a `Parameter`/`Variable` is"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWRUN6Qsv5OO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param = net.W_1\n",
        "print(param)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HTWFlL-v5OQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"## this is the tensor\")\n",
        "print(param.data)\n",
        "\n",
        "print(\"\\n## this is the tensor's gradient\")\n",
        "print(param.grad)\n",
        "# notice, the gradient is undefined because we have not yet run a backward pass\n",
        "\n",
        "print(\"\\n## is it a leaf in the graph?\")\n",
        "print(param.is_leaf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twEBABbjv5OR",
        "colab_type": "text"
      },
      "source": [
        "## Excluding subgraphs from backward propagation\n",
        "\n",
        "`Variable`s have two properties which allow you to define if a graph is going to be used for training or inference, `requires_grad` and `volatile`.\n",
        "During training we might not want to compute the gradients for all layers, e.g. if we have a pretrained model or embeddings that we do not want to change the values of.\n",
        "\n",
        "To compute gradients we need to store activations and compute the backward pass for the given layer.\n",
        "Setting `requires_grad = False` will allow you to circumvent these properties.\n",
        "If any paramater in an operation / layer requires gradient then the entire output of the operation will also require gradient.\n",
        "\n",
        "The `volatile` property is mostly used when you want to run inference with your model, and if it is set to `True` the entire graph will not require gradient. This means that you expect to never call `.backward()` on the network.\n",
        "\n",
        "See http://pytorch.org/docs/master/notes/autograd.html for an in-depth explanation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAB8jV8rv5OS",
        "colab_type": "text"
      },
      "source": [
        "## Test network\n",
        "\n",
        "To use our network we can simply call our graph, and it will dynamically be created. Here is an example of running the network's forward pass."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRlvEFN8v5OS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_neurons = 5\n",
        "X = Variable(torch.randn(num_neurons, num_features))\n",
        "\n",
        "print('input')\n",
        "print(X)\n",
        "print(X.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJvxq4nPv5OT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pass through the NN\n",
        "print('\\noutput')\n",
        "print(net(X))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzkPxipPv5OV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('\\nWEIGHT')\n",
        "\n",
        "param = net.W_1\n",
        "print(param)\n",
        "\n",
        "print('\\nBIAS')\n",
        "print(net.b_1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Y-ghqHwv5OW",
        "colab_type": "text"
      },
      "source": [
        "Parameters is a special case of Variable "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJ9DVgyvv5OX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# let's take a look at the gradients\n",
        "for p in net.parameters():\n",
        "    print('DATA >>>', p.data)\n",
        "    print('GRADIENT >>>', p.grad)\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWplTQY7v5OY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = Variable(torch.randn(7, num_features))\n",
        "out = net(X)\n",
        "print(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8CpjE_lv5Oa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we need to give a tensor of gradients to .backward,\n",
        "# we give a dummy tensor\n",
        "out.backward(torch.randn(7, num_features))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiFEfL0jv5Ob",
        "colab_type": "text"
      },
      "source": [
        "for details on `.backward()`, see http://pytorch.org/docs/master/autograd.html#torch.autograd.backward"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9B-P5kwv5Ob",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# let's take a look at the gradients\n",
        "for p in net.parameters():\n",
        "    print('DATA >>>', p.data)\n",
        "    print('GRADIENT >>>', p.grad)\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6lbgoN4v5Of",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ok, let's try and zero the accumulated gradients\n",
        "net.zero_grad()\n",
        "for p in net.parameters():\n",
        "    print('DATA >>>', p.data)\n",
        "    print('GRADIENT >>>', p.grad)\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyrqFc4qv5Og",
        "colab_type": "text"
      },
      "source": [
        "## Loss function\n",
        "\n",
        "Let's define a custom loss function to compute how good our graph is doing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRFM-ssWv5Og",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ys - estimated # ts - target output \n",
        "def cross_entropy(ys, ts):      \n",
        "    # computing cross entropy per sample\n",
        "    cross_entropy = -torch.sum(ts * torch.log(ys), dim=1, keepdim=False)        \n",
        "    # averaging over samples\n",
        "    average = torch.mean(cross_entropy)        \n",
        "    return average"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnlM_iHHv5Oi",
        "colab_type": "text"
      },
      "source": [
        "To train our neural network we need to update the parameters in direction of the negative gradient w.r.t the cost function we defined earlier.\n",
        "We can use [`torch.optim`](http://pytorch.org/docs/master/optim.html) to get the gradients with some update rule for all parameters in the network.\n",
        "\n",
        "Heres a small animation of gradient descent: http://imgur.com/a/Hqolp, which also illustrates which challenges optimizers might face, e.g. saddle points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbTamG-7v5Oi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "optimizer = optim.SGD(net.parameters(), lr = 0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brZjiOLqv5Ok",
        "colab_type": "text"
      },
      "source": [
        "Next, we make the prediction functions, such that we can get an accuracy measure over a batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLJvpyS9v5Ok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(ys, ts):    \n",
        "    # making a one-hot encoded vector of correct (1) and incorrect (0) predictions\n",
        "    correct_prediction = torch.eq(torch.max(ys, 1)[1], torch.max(ts, 1)[1])        \n",
        "    # averaging the one-hot encoded vector\n",
        "    Average = torch.mean(correct_prediction.float())\n",
        "    return Average"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBFhIJBGv5Om",
        "colab_type": "text"
      },
      "source": [
        "The next step is to utilize our `optimizer` repeatedly in order to optimize our weights `W_1` and `b_1` to make the best possible linear seperation of the half moon dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bXkX32Nv5Om",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# number of training passses\n",
        "num_epochs = 1000\n",
        "\n",
        "# store loss and accuracy for information\n",
        "train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
        "\n",
        "def pred(X):\n",
        "    \"\"\" Compute graph's prediction and return numpy array    \n",
        "    Parameters\n",
        "    ----------\n",
        "    X : numpy.ndarray    \n",
        "    Returns\n",
        "    -------\n",
        "    numpy.ndarray\n",
        "    \"\"\"\n",
        "    X = Variable(torch.from_numpy(X))\n",
        "         \n",
        "    # forward pass \n",
        "    y = net(X)    \n",
        "    return y.data.numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7blwIUHv5On",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot boundary on testset before training session\n",
        "plot_decision_boundary(lambda x: pred(x), X_te, y_te)\n",
        "plt.title(\"Untrained Classifier\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8OB8xGCv5Op",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pdb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDNi7mqLv5Oq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training loop\n",
        "for e in range(num_epochs):\n",
        "    # get training input and expected output as torch Variables and \n",
        "    # make sure type is correct\n",
        "    \n",
        "    tr_input = Variable(torch.from_numpy(X_tr))\n",
        "    tr_targets = Variable(torch.from_numpy(onehot(y_tr, num_output))).float()\n",
        "    \n",
        "    # zero accumulated gradients in parameters\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    #+++++++++++ TRAIN STEP ++++++++++++++++\n",
        "    # predict by running forward pass\n",
        "    tr_output = net(tr_input)     \n",
        "    # pdb.set_trace()\n",
        "    \n",
        "    # compute cross entropy loss\n",
        "    tr_loss = cross_entropy(tr_output, tr_targets)    \n",
        "   \n",
        "    # compute gradients given loss\n",
        "    tr_loss.backward()   \n",
        "    \n",
        "    # update the parameters given the computed gradients\n",
        "    # Performs a single optimization step\n",
        "    optimizer.step()\n",
        "    \n",
        "    train_acc = accuracy(tr_output, tr_targets)\n",
        "    \n",
        "    # store training loss\n",
        "    train_losses.append(tr_loss.data.numpy())\n",
        "    train_accs.append(train_acc)\n",
        "    #+++++++++++++++++++++++++++++++++++++++\n",
        "    \"\"\"    at each step/epoch USE the trained net() \n",
        "    to evaluate it on the validation set\n",
        "    - Note however that the update is not happening\n",
        "    after the validation step.\n",
        "    \n",
        "    tr_loss.backward() ->>> does the backprop \n",
        "    optimizer.step() ->>> does the weight update    \n",
        "    \"\"\"    \n",
        "    #+++++++++++ VALIDATION STEP ++++++++++++++++\n",
        "    # get validation input and expected output as torch Variables \n",
        "    # and make sure type is correct\n",
        "    val_input = Variable(torch.from_numpy(X_val))\n",
        "    val_targets = Variable(torch.from_numpy(onehot(y_val, num_output))).float()\n",
        "    \n",
        "    # predict with validation input\n",
        "    val_output = net(val_input)\n",
        "    \n",
        "    # compute loss and accuracy\n",
        "    val_loss = cross_entropy(val_output, val_targets)\n",
        "    val_acc = accuracy(val_output, val_targets)\n",
        "    \n",
        "    # store loss and accuracy\n",
        "    val_losses.append(val_loss.data.numpy())\n",
        "    val_accs.append(val_acc.data.numpy())\n",
        "    #+++++++++++++++++++++++++++++++++++++++\n",
        "        \n",
        "    if e % 100 == 0:\n",
        "        print(\"Epoch %i, \"\n",
        "              \"Train Cost: %0.3f\"\n",
        "              \"\\tVal Cost: %0.3f\"\n",
        "              \"\\t Val acc: %0.3f\" % (e, \n",
        "                                     train_losses[-1],\n",
        "                                     val_losses[-1],\n",
        "                                     val_accs[-1]))        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qog6d9lkv5Os",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get test input and expected output\n",
        "te_input = Variable(torch.from_numpy(X_te))\n",
        "te_targets = Variable(torch.from_numpy(onehot(y_te, num_output))).float()\n",
        "\n",
        "# predict on testset\n",
        "te_output = net(te_input)\n",
        "\n",
        "# compute loss and accuracy\n",
        "te_loss = cross_entropy(te_output, te_targets)\n",
        "te_acc = accuracy(te_output, te_targets)\n",
        "print(\"\\nTest Cost: %0.3f\\tTest Accuracy: %0.3f\" % (te_loss.data.numpy(), te_acc.data.numpy()))\n",
        "\n",
        "# plot boundary on testset after training session\n",
        "plot_decision_boundary(lambda x: pred(x), X_te, y_te)\n",
        "plt.title(\"Trained Classifier\")\n",
        "\n",
        "plt.figure()\n",
        "epoch = np.arange(len(train_losses))\n",
        "plt.plot(epoch, train_losses, 'r', label='Train Loss')\n",
        "plt.plot(epoch, val_losses, 'b', label='Val Loss')\n",
        "plt.legend()\n",
        "plt.xlabel('Updates')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(epoch, train_accs, 'r', label='Train Acc')\n",
        "plt.plot(epoch, val_accs, 'b', label='Val Acc')\n",
        "plt.legend()\n",
        "plt.xlabel('Updates')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhqUk6wDv5Ot",
        "colab_type": "text"
      },
      "source": [
        "-----------\n",
        "### Part 3 - Assignments\n",
        "\n",
        "1. A linear logistic classifier is only able to create a linear decision boundary. Change the Logistic classifier into a (nonlinear) Neural network by inserting a dense hidden layer between the input and output layers of the model\n",
        " \n",
        "2. Experiment with multiple hidden layers or more / less hidden units. What happens to the decision boundary?\n",
        " \n",
        "3. Overfitting: When increasing the number of hidden layers / units the neural network will fit the training data better by creating a highly nonlinear decision boundary. If the model is to complex it will often generalize poorly to new data (validation and test set). Can you observe this from the training and validation errors? \n",
        " \n",
        "4. We used the vanilla stocastic gradient descent algorithm for parameter updates. This is usually slow to converge and more sophisticated pseudo-second-order methods usually works better. Try changing the optimizer to [adam or momentum](http://pytorch.org/docs/master/optim.html#torch.optim.Adam)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iB573xhGv5Ou",
        "colab_type": "text"
      },
      "source": [
        "--------------------------------------\n",
        "# PART 4 - MNIST Feed-Forward Neural Networks "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIz9b-7pv5Ov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.nn.init as init\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYiO8RxZv5Oy",
        "colab_type": "text"
      },
      "source": [
        "### MNIST dataset\n",
        "MNIST is a dataset that is often used for benchmarking. The MNIST dataset consists of 70,000 images of handwritten digits from 0-9. The dataset is split into a 50,000 images training set, 10,000 images validation set and 10,000 images test set. The images are 28x28 pixels, where each pixel represents a normalised value between 0-255 `(0=black and 255=white)`.\n",
        "\n",
        "http://yann.lecun.com/exdb/mnist/\n",
        "\n",
        "### Primer\n",
        "We use a feedforward neural network to classify the 28x28 mnist images. `num_features` is therefore $28 * 28=784$, i.e. we represent each image as a vector. The ordering of the pixels in the vector does not matter, so we could permutate all images using the same permutation and still get the same performance. (You are of course encouraged to try this using ``numpy.random.permutation`` to get a random permutation. This task is therefore called the _permutation invariant_ MNIST. Obviously this throws away a lot of structure in the data. In the next module we'll fix this with the convolutional neural network wich encodes prior knowledgde about data that has either spatial or temporal structure.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZQ44n4Wv5Oy",
        "colab_type": "text"
      },
      "source": [
        "### MNIST\n",
        "First let's load the MNIST dataset and plot a few examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8CsyA9rv5Oz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install wget\n",
        "import wget\n",
        "wget.download('https://github.com/maximai/placeholder_repo/raw/master/mnist.npz')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lHm-wd9v5O0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#To speed up training we'll only work on a subset of the data\n",
        "data = np.load('mnist.npz')\n",
        "num_classes = 10\n",
        "\n",
        "data['X_train'].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUfPbFslv5O1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#To speed up training we'll only work on a subset of the data\n",
        "x_train = data['X_train'][:1000].astype('float32')\n",
        "targets_train = data['y_train'][:1000].astype('int32')\n",
        "\n",
        "x_valid = data['X_valid'][:500].astype('float32')\n",
        "targets_valid = data['y_valid'][:500].astype('int32')\n",
        "\n",
        "x_test = data['X_test'][:500].astype('float32')\n",
        "targets_test = data['y_test'][:500].astype('int32')\n",
        "\n",
        "print(\"Information on dataset\")\n",
        "print(\"x_train\", x_train.shape)\n",
        "print(\"targets_train\", targets_train.shape)\n",
        "print(\"x_valid\", x_valid.shape)\n",
        "print(\"targets_valid\", targets_valid.shape)\n",
        "print(\"x_test\", x_test.shape)\n",
        "print(\"targets_test\", targets_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWw5rd0ev5O3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E34Z5dFSv5O4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#plot a few MNIST examples\n",
        "idx, dim, classes = 0, 28, 10\n",
        "# create empty canvas\n",
        "canvas = np.zeros((dim*classes, classes*dim))\n",
        "\n",
        "# fill with tensors\n",
        "for i in range(classes):\n",
        "    for j in range(classes):\n",
        "        canvas[i*dim:(i+1)*dim, j*dim:(j+1)*dim] = x_train[idx].reshape((dim, dim))\n",
        "        idx += 1\n",
        "\n",
        "# visualize matrix of tensors as gray scale image\n",
        "plt.figure(figsize=(4, 4))\n",
        "plt.axis('off')\n",
        "plt.imshow(canvas, cmap='gray')\n",
        "plt.title('MNIST handwritten digits')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6P2tZLSFv5O5",
        "colab_type": "text"
      },
      "source": [
        "## Model\n",
        "\n",
        "One of the large challenges in deep learning is the amount of hyperparameters that needs to be selected, and the lack of a good principled way of selecting them.\n",
        "Hyperparameters can be found by experience (guessing) or some search procedure (often quite slow).\n",
        "Random search is easy to implement and performs decent: http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf . \n",
        "More advanced search procedures include [Spearmint](https://github.com/JasperSnoek/spearmint) and many others.\n",
        "\n",
        "**In practice a lot of trial and error is almost always involved.** This can be frustrating and time consuming, but the best thing to do is to think as a scientist, and go about it in a ordered manner --> monitor as much as you can, take notes, and be deliberate!\n",
        "\n",
        "Below are some guidelines that you can use as a starting point to some of the most important hyperparameters. \n",
        "(*regularization* is also very important, but will be covered later.)\n",
        "\n",
        "\n",
        "### Ballpark estimates of hyperparameters\n",
        "__Number of hidden units and network structure:__\n",
        "You'll have to experiment. One rarely goes below 512 units for feedforward networks (unless your are training on CPU...).\n",
        "There's some research into stochastic depth networks: https://arxiv.org/pdf/1603.09382v2.pdf, but in general this is trial and error.\n",
        "\n",
        "__Parameter initialization:__\n",
        "Parameter initialization is extremely important.\n",
        "PyTorch has a lot of different initializers, check the [PyTorch API](http://pytorch.org/docs/master/nn.html#torch-nn-init). Often used initializer are\n",
        "1. Kaming He\n",
        "2. Xavier Glorot\n",
        "3. Uniform or Normal with small scale (0.1 - 0.01)\n",
        "4. Orthogonal (this usually works very well for RNNs)\n",
        "\n",
        "Bias is nearly always initialized to zero using the [torch.nn.init.constant(tensor, val)](http://pytorch.org/docs/master/nn.html#torch.nn.init.constant)\n",
        "\n",
        "__Mini-batch size:__\n",
        "Usually people use 16-256. Bigger is not allways better. With smaller mini-batch size you get more updates and your model might converge faster. Also small batch sizes use less memory, which means you can train a model with more parameters.\n",
        "\n",
        "__Nonlinearity:__ [The most commonly used nonliearities are](http://pytorch.org/docs/master/nn.html#non-linear-activations)\n",
        "1. ReLU\n",
        "2. Leaky ReLU\n",
        "3. Elu\n",
        "3. Sigmoid squash the output [0, 1], and are used if your output is binary (not used in the hidden layers)\n",
        "4. Tanh is similar to sigmoid, but squashes in [-1, 1]. It is rarely used any more.\n",
        "4. Softmax normalizes the the output to 1, and is used as output if you have a classification problem\n",
        "\n",
        "See the plot below.\n",
        "\n",
        "__Optimizer and learning rate:__\n",
        "1. SGD + Momentum: learning rate 1.0 - 0.1 \n",
        "2. ADAM: learning rate 3*1e-4 - 1e-5\n",
        "3. RMSPROP: somewhere between SGD and ADAM\n",
        "\n",
        "SGD with momentum is method which helps accelerate gradients vectors in the right directions, thus leading to faster converging. It is one of the most popular optimization algorithms and many state-of-the-art models are trained using it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d33Yh-a6v5O6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Illustrate different output units\n",
        "x = np.linspace(-6, 6, 100)\n",
        "\n",
        "units = {\n",
        "    \"ReLU\": lambda x: np.maximum(0, x),\n",
        "    \"Leaky ReLU\": lambda x: np.maximum(0, x) + 0.1 * np.minimum(0, x),\n",
        "    \"Elu\": lambda x: (x > 0) * x + (1 - (x > 0)) * (np.exp(x) - 1),\n",
        "    \"Sigmoid\": lambda x: (1 + np.exp(-x))**(-1),\n",
        "    \"tanh\": lambda x: (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
        "    }\n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "[plt.plot(x, unit(x), label=unit_name, lw=2) for unit_name, unit in units.items()]\n",
        "plt.legend(loc=2, fontsize=16)\n",
        "plt.title('Non-linearities', fontsize=20)\n",
        "plt.ylim([-2, 5])\n",
        "plt.xlim([-6, 6])\n",
        "# assert that all class probablities sum to one\n",
        "softmax = lambda x: np.exp(x) / np.sum(np.exp(x))\n",
        "print(\"softmax should sum to one (approxiamtely):\", np.sum(softmax(x)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btZpxH0ev5O7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Hyperparameters\n",
        "num_classes = 10\n",
        "num_l1 = 512\n",
        "num_features = x_train.shape[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxSpI4BMv5O9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8LzPUsQv5O-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define network\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, num_features, num_hidden, num_output):\n",
        "        super(Net, self).__init__()  \n",
        "        \n",
        "        # input layer\n",
        "        self.W_1 = Parameter(init.xavier_normal_(torch.Tensor(num_hidden, num_features)))\n",
        "        self.b_1 = Parameter(init.constant_(torch.Tensor(num_hidden), 0))\n",
        "        \n",
        "        # hidden layer\n",
        "        self.W_2 = Parameter(init.xavier_normal_(torch.Tensor(num_output, num_hidden)))\n",
        "        self.b_2 = Parameter(init.constant_(torch.Tensor(num_output), 0))\n",
        "        \n",
        "        # define activation function in constructor\n",
        "        self.activation = torch.nn.ELU()\n",
        "\n",
        "    def forward(self, x):        \n",
        "        x = F.linear(x, self.W_1, self.b_1)\n",
        "        x = self.activation(x)\n",
        "        \n",
        "        x = F.linear(x, self.W_2, self.b_2)\n",
        "        return F.softmax(x, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDvzF9TPv5PA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = Net(num_features, num_l1, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQM912xRv5PB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.SGD(net.parameters(), lr=0.1)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50V5aBKzv5PC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Test the forward pass with dummy data\n",
        "x = np.random.normal(0, 1, (45, dim*dim)).astype('float32')\n",
        "\n",
        "x = torch.from_numpy(x)\n",
        "\n",
        "out = net(Variable(x)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00c-vn_Yv5PD",
        "colab_type": "text"
      },
      "source": [
        "# Build the training loop\n",
        "\n",
        "We train the network by calculating the gradient w.r.t the cost function and update the parameters in direction of the negative gradient. \n",
        "\n",
        "\n",
        "When training neural network you always use mini batches. Instead of calculating the average gradient using the entire dataset you approximate the gradient using a mini-batch of typically 16 to 256 samples. The paramters are updated after each mini batch. Networks converge much faster using mini batches because the parameters are updated more often.\n",
        "\n",
        "We build a loop that iterates over the training data. Remember that the parameters are updated each time ``optimizer.step()`` is called."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnSrdMosv5PE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we could have done this ourselves,\n",
        "# but we should be aware of sklearn and it's tools\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# setting hyperparameters and gettings epoch sizes\n",
        "batch_size = 100\n",
        "num_epochs = 100\n",
        "\n",
        "num_samples_train = x_train.shape[0]\n",
        "num_batches_train = num_samples_train // batch_size\n",
        "\n",
        "num_samples_valid = x_valid.shape[0]\n",
        "num_batches_valid = num_samples_valid // batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ss_wU4lyv5PH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# setting up lists for handling loss/accuracy\n",
        "train_acc, train_loss = [], []\n",
        "valid_acc, valid_loss = [], []\n",
        "test_acc, test_loss = [], []\n",
        "cur_loss = 0\n",
        "losses = []\n",
        "\n",
        "get_slice = lambda i, size: range(i * size, (i + 1) * size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJmQArVSv5PI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(num_epochs):\n",
        "    # Forward -> Backprob -> Update weights\n",
        "    ## Train\n",
        "    cur_loss = 0\n",
        "    net.train()    \n",
        "    for i in range(num_batches_train):\n",
        "        slce = get_slice(i, batch_size)\n",
        "        x_batch = Variable(torch.from_numpy(x_train[slce]))\n",
        "        # forward passing \n",
        "        output = net(x_batch)\n",
        "        \n",
        "        # compute gradients given loss\n",
        "        target_batch = Variable(torch.from_numpy(targets_train[slce]).long())\n",
        "        \n",
        "        # loss function \n",
        "        batch_loss  = criterion(output, target_batch)        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # backpropogate \n",
        "        batch_loss.backward()\n",
        "        \n",
        "        # update weight \n",
        "        optimizer.step()        \n",
        "        cur_loss += batch_loss   \n",
        "        \n",
        "    losses.append(cur_loss / batch_size)\n",
        "    \n",
        "    net.eval()    \n",
        "    ## Evaluate training\n",
        "    train_preds, train_targs = [], []\n",
        "    \n",
        "    for i in range(num_batches_train):\n",
        "        slce = get_slice(i, batch_size)\n",
        "        x_batch = Variable(torch.from_numpy(x_train[slce]))\n",
        "        \n",
        "        output = net(x_batch)\n",
        "        preds = torch.max(output, 1)[1]\n",
        "        \n",
        "        train_targs += list(targets_train[slce])\n",
        "        train_preds += list(preds.data.numpy())\n",
        "    \n",
        "    ## Evaluate validation\n",
        "    val_preds, val_targs = [], []\n",
        "    for i in range(num_batches_valid):\n",
        "        slce = get_slice(i, batch_size)\n",
        "        x_batch = Variable(torch.from_numpy(x_valid[slce]))        \n",
        "        output = net(x_batch)\n",
        "        \n",
        "        preds = torch.max(output, 1)[1]\n",
        "        val_preds += list(preds.data.numpy())\n",
        "        val_targs += list(targets_valid[slce])\n",
        "\n",
        "    train_acc_cur = accuracy_score(train_targs, train_preds)\n",
        "    valid_acc_cur = accuracy_score(val_targs, val_preds)\n",
        "    \n",
        "    train_acc.append(train_acc_cur)\n",
        "    valid_acc.append(valid_acc_cur)\n",
        "    \n",
        "    if epoch % 10 == 0:\n",
        "        print(\"Epoch %2i : Train Loss %f , Train acc %f, Valid acc %f\" % (\n",
        "                epoch+1, losses[-1], train_acc_cur, valid_acc_cur))\n",
        "\n",
        "epoch = np.arange(len(train_acc))\n",
        "plt.figure()\n",
        "plt.plot(epoch, train_acc, 'r', epoch, valid_acc, 'b')\n",
        "plt.legend(['Train Accucary','Validation Accuracy'])\n",
        "plt.xlabel('Updates'), plt.ylabel('Acc')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1ElblEkv5PJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for ii in net.parameters():\n",
        "    print(ii.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxbEJqIwv5PK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test = data['X_test'][:500].astype('float32')\n",
        "targets_test = data['y_test'][:500].astype('int32')\n",
        "\n",
        "xtest = torch.from_numpy(x_test)\n",
        "\n",
        "tout = net(xtest)\n",
        "\n",
        "tout.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vssUwYepv5PM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = torch.from_numpy(targets_test)\n",
        "\n",
        "preds = torch.max(tout, 1)[1]\n",
        "testacc = accuracy_score(preds, y)\n",
        "\n",
        "preds[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6C4FmQRgv5PN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f45GE5NMv5PO",
        "colab_type": "text"
      },
      "source": [
        "------------------\n",
        "### Part 4 - Assignments \n",
        "\n",
        "Try and add these modifications (might require some Googleing -- an important skill in deep learning):\n",
        "- Kaiming He initialization instead of Xavier Glorot\n",
        "- add an extra layer\n",
        "- use the relu activation function\n",
        "- add momentum to the optimizer\n",
        "- use the ADAM optimizer instead of stochastic gradient descent\n",
        "\n",
        "### Advanced - Regularization\n",
        "\n",
        "Regularization is VERY important in practice and is used practically every time.\n",
        "Many important results are completely dependent on cleaver use of regularization, and it is something you need to become familiar with if you want to work with deep learning.\n",
        "\n",
        "- add L1 or L2 weight regularization (aka. weight decay) \n",
        "- add dropout to the network (**note** the `net.train()` and `net.eval()` are already in the code)\n",
        "- add batchnorm\n",
        "\n",
        "__Pointers on regularization hyperparameter:__\n",
        "1. L2 and [L1 regularization](http://pytorch.org/docs/master/nn.html#torch.nn.L1Loss) (weight decay of optimization functions) \n",
        "  - Normal ranges: 1e-4  -  1e-8\n",
        "1. [Dropout](http://pytorch.org/docs/master/nn.html?highlight=dropout#torch.nn.Dropout). Dropout rate 0.1-0.5\n",
        "  - Remember to pick the correct version according to the input dimensionality\n",
        "  - **NOTE** call `net.train()` before training to activate random dropout, and call `net.eval()` to deactivate dropout while validating or running inference with model.\n",
        "1. [Batchnorm](http://pytorch.org/docs/master/nn.html#torch.nn.BatchNorm1d): Batchnorm also acts as a regularizer - Often very useful (faster and better convergence)\n",
        "  - Remember to pick the correct version according to the input dimensionality\n",
        "  - **NOTE** call `net.train()` before training to activate, and call `net.eval()` to have a non-stochastic variant while validating or running inference with model.\n",
        "    "
      ]
    }
  ]
}